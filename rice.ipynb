{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using seed() so that every time we execute the code we would have same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,we have to import tensorflow module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using seed() so that every time we execute the code we would have same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,we have to import tensorflow module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimensions\n",
    "img_width, img_height = 256, 256\n",
    "input_shape = (img_width, img_height, 3)\n",
    "num_classes = 6 #Class indices of Dataset i.e the number of folders our data set consists of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset path \n",
    "data_dir=\"C:\\\\Users\\\\chand\\\\Music\\\\datasetfordp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory exists\n"
     ]
    }
   ],
   "source": [
    "# Checking  if directories exist\n",
    "assert os.path.exists(data_dir), \"Dataset directory does not exist\"\n",
    "print(\"Dataset directory exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating data for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,By using flow_from_directory we extract each data from train_datagen and then we will be training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1680 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 420 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the class indices or the number of class the dataset consists of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class indices: {'bacterial_leaf_blight': 0, 'brown_spot': 1, 'healthy': 2, 'leaf_blast': 3, 'leaf_scald': 4, 'narrow_brown_spot': 5}\n"
     ]
    }
   ],
   "source": [
    "print(\"Class indices:\", train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the  number of samples that have been categorised as train and validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1680 training samples\n",
      "Found 420 validation samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {train_generator.samples} training samples\")\n",
    "print(f\"Found {validation_generator.samples} validation samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that  there are samples in the generators,we use assert to check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_generator.samples > 0, \"No training samples found\"\n",
    "assert validation_generator.samples > 0, \"No validation samples found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function for cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function call for the cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chand\\anaconda3\\envs\\exsel\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn_model = create_cnn_model(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate steps_per_epoch and validation_steps\n",
    "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
    "validation_steps = validation_generator.samples // validation_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_per_epoch: 52\n",
      "validation_steps: 13\n"
     ]
    }
   ],
   "source": [
    "print(f\"steps_per_epoch: {steps_per_epoch}\")\n",
    "print(f\"validation_steps: {validation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure steps_per_epoch and validation_steps are greater than 0\n",
    "if steps_per_epoch == 0:\n",
    "    steps_per_epoch = 1\n",
    "    print(\"Adjusted steps_per_epoch to 1\")\n",
    "\n",
    "if validation_steps == 0:\n",
    "    validation_steps = 1\n",
    "    print(\"Adjusted validation_steps to 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chand\\anaconda3\\envs\\exsel\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.2908 - loss: 2.3711 - val_accuracy: 0.4928 - val_loss: 1.2433\n",
      "Epoch 2/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4062 - loss: 1.3600 - val_accuracy: 0.5000 - val_loss: 2.2092\n",
      "Epoch 3/35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chand\\anaconda3\\envs\\exsel\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 2s/step - accuracy: 0.5383 - loss: 1.1219 - val_accuracy: 0.5601 - val_loss: 1.1071\n",
      "Epoch 4/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6250 - loss: 0.9456 - val_accuracy: 0.2500 - val_loss: 1.1431\n",
      "Epoch 5/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 2s/step - accuracy: 0.6075 - loss: 1.0259 - val_accuracy: 0.5601 - val_loss: 1.0122\n",
      "Epoch 6/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5938 - loss: 0.8162 - val_accuracy: 0.7500 - val_loss: 0.6570\n",
      "Epoch 7/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 2s/step - accuracy: 0.6650 - loss: 0.8703 - val_accuracy: 0.6659 - val_loss: 0.9516\n",
      "Epoch 8/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 838us/step - accuracy: 0.6562 - loss: 0.8790 - val_accuracy: 0.5000 - val_loss: 0.8749\n",
      "Epoch 9/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 2s/step - accuracy: 0.7150 - loss: 0.7639 - val_accuracy: 0.6538 - val_loss: 0.9230\n",
      "Epoch 10/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7812 - loss: 0.6753 - val_accuracy: 0.5000 - val_loss: 1.1413\n",
      "Epoch 11/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 2s/step - accuracy: 0.7369 - loss: 0.6780 - val_accuracy: 0.6538 - val_loss: 0.9561\n",
      "Epoch 12/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 824us/step - accuracy: 0.7500 - loss: 0.8037 - val_accuracy: 0.5000 - val_loss: 1.8218\n",
      "Epoch 13/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 0.7554 - loss: 0.6660 - val_accuracy: 0.7115 - val_loss: 0.8075\n",
      "Epoch 14/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7500 - loss: 0.7052 - val_accuracy: 0.7500 - val_loss: 0.3425\n",
      "Epoch 15/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.7732 - loss: 0.6048 - val_accuracy: 0.6971 - val_loss: 0.8544\n",
      "Epoch 16/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 651us/step - accuracy: 0.8125 - loss: 0.5238 - val_accuracy: 0.5000 - val_loss: 0.7213\n",
      "Epoch 17/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 2s/step - accuracy: 0.7928 - loss: 0.5536 - val_accuracy: 0.7091 - val_loss: 0.6559\n",
      "Epoch 18/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 936us/step - accuracy: 0.6562 - loss: 0.6822 - val_accuracy: 0.7500 - val_loss: 0.7101\n",
      "Epoch 19/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2s/step - accuracy: 0.8124 - loss: 0.4912 - val_accuracy: 0.7692 - val_loss: 0.5759\n",
      "Epoch 20/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8750 - loss: 0.4435 - val_accuracy: 0.5000 - val_loss: 0.7976\n",
      "Epoch 21/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - accuracy: 0.8139 - loss: 0.4667 - val_accuracy: 0.7260 - val_loss: 0.6410\n",
      "Epoch 22/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714us/step - accuracy: 0.9062 - loss: 0.4605 - val_accuracy: 0.5000 - val_loss: 1.3546\n",
      "Epoch 23/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.8535 - loss: 0.4042 - val_accuracy: 0.7740 - val_loss: 0.5957\n",
      "Epoch 24/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7500 - loss: 0.6030 - val_accuracy: 1.0000 - val_loss: 0.2037\n",
      "Epoch 25/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 2s/step - accuracy: 0.8605 - loss: 0.3902 - val_accuracy: 0.7644 - val_loss: 0.5865\n",
      "Epoch 26/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 626us/step - accuracy: 0.8125 - loss: 0.3964 - val_accuracy: 0.7500 - val_loss: 0.4116\n",
      "Epoch 27/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 2s/step - accuracy: 0.8566 - loss: 0.3987 - val_accuracy: 0.7308 - val_loss: 0.6292\n",
      "Epoch 28/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8750 - loss: 0.3187 - val_accuracy: 1.0000 - val_loss: 0.1522\n",
      "Epoch 29/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 2s/step - accuracy: 0.8688 - loss: 0.3664 - val_accuracy: 0.7764 - val_loss: 0.5973\n",
      "Epoch 30/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7812 - loss: 0.4512 - val_accuracy: 0.7500 - val_loss: 0.3571\n",
      "Epoch 31/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 2s/step - accuracy: 0.8425 - loss: 0.4222 - val_accuracy: 0.7692 - val_loss: 0.7074\n",
      "Epoch 32/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 976us/step - accuracy: 0.8125 - loss: 0.4582 - val_accuracy: 0.5000 - val_loss: 0.8425\n",
      "Epoch 33/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 1s/step - accuracy: 0.8732 - loss: 0.3195 - val_accuracy: 0.7572 - val_loss: 0.6535\n",
      "Epoch 34/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9062 - loss: 0.2189 - val_accuracy: 1.0000 - val_loss: 0.2199\n",
      "Epoch 35/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - accuracy: 0.8809 - loss: 0.3439 - val_accuracy: 0.7861 - val_loss: 0.7089\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history=cnn_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 611ms/step - accuracy: 0.7871 - loss: 0.5876\n",
      "Validation Accuracy: 80.77%\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "print(\"Evaluating model...\")\n",
    "val_loss, val_accuracy = cnn_model.evaluate(validation_generator, steps=validation_generator.samples //validation_generator.batch_size)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Load and Preprocess the Image using Pillow\n",
    "from PIL import Image\n",
    "def load_and_preprocess_image(image_path, target_size=(256, 256)):\n",
    "    # Load the image\n",
    "    img = Image.open(image_path)\n",
    "    # Resize the image\n",
    "    img = img.resize(target_size)\n",
    "    # Convert the image to a numpy array\n",
    "    img_array = np.array(img)\n",
    "    # Add batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    # Scale the image values to [0, 1]\n",
    "    img_array = img_array.astype('float32') / 255.\n",
    "    return img_array\n",
    "\n",
    "# Function to Predict the Class of an Image\n",
    "def predict_image_class(cnn_model, image_path, class_indices):\n",
    "    preprocessed_img = load_and_preprocess_image(image_path)\n",
    "    predictions = cnn_model.predict(preprocessed_img)\n",
    "    predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "    predicted_class_name = class_indices[predicted_class_index]\n",
    "    return predicted_class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from class indices to class names\n",
    "class_indices = {v: k for k, v in train_generator.class_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'bacterial_leaf_blight',\n",
       " 1: 'brown_spot',\n",
       " 2: 'healthy',\n",
       " 3: 'leaf_blast',\n",
       " 4: 'leaf_scald',\n",
       " 5: 'narrow_brown_spot'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the class names as json file\n",
    "import json\n",
    "json.dump(class_indices, open('class_indices.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\chand\\\\OneDrive\\\\Desktop\\\\exsel\\\\train_images\\\\bacterial_leaf_blight\\\\100133.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mchand\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mexsel\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtrain_images\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbacterial_leaf_blight\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m100133.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m predicted_class_name \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_image_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Output the result\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Class Name:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted_class_name)\n",
      "Cell \u001b[1;32mIn[30], line 18\u001b[0m, in \u001b[0;36mpredict_image_class\u001b[1;34m(cnn_model, image_path, class_indices)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_image_class\u001b[39m(cnn_model, image_path, class_indices):\n\u001b[1;32m---> 18\u001b[0m     preprocessed_img \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m cnn_model\u001b[38;5;241m.\u001b[39mpredict(preprocessed_img)\n\u001b[0;32m     20\u001b[0m     predicted_class_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m, in \u001b[0;36mload_and_preprocess_image\u001b[1;34m(image_path, target_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_preprocess_image\u001b[39m(image_path, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Load the image\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Resize the image\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mresize(target_size)\n",
      "File \u001b[1;32mc:\\Users\\chand\\anaconda3\\envs\\exsel\\Lib\\site-packages\\PIL\\Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\chand\\\\OneDrive\\\\Desktop\\\\exsel\\\\train_images\\\\bacterial_leaf_blight\\\\100133.jpg'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "image_path = 'c:\\\\Users\\\\chand\\\\OneDrive\\\\Desktop\\\\exsel\\\\train_images\\\\bacterial_leaf_blight\\\\100133.jpg'\n",
    "predicted_class_name = predict_image_class(cnn_model, image_path, class_indices)\n",
    "\n",
    "# Output the result\n",
    "print(\"Predicted Class Name:\", predicted_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "cnn_model.save('C:\\\\Users\\\\chand\\\\OneDrive\\\\Desktop\\\\DP project\\\\Rice_leaf_disease_detection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimensions\n",
    "img_width, img_height = 256, 256\n",
    "input_shape = (img_width, img_height, 3)\n",
    "num_classes = 6 #Class indices of Dataset i.e the number of folders our data set consists of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset path \n",
    "data_dir=\"C:\\\\Users\\\\chand\\\\OneDrive\\\\Desktop\\\\datasetfordp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory exists\n"
     ]
    }
   ],
   "source": [
    "# Checking  if directories exist\n",
    "assert os.path.exists(data_dir), \"Dataset directory does not exist\"\n",
    "print(\"Dataset directory exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating data for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,By using flow_from_directory we extract each data from train_datagen and then we will be training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1680 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 420 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the class indices or the number of class the dataset consists of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class indices: {'bacterial_leaf_blight': 0, 'brown_spot': 1, 'healthy': 2, 'leaf_blast': 3, 'leaf_scald': 4, 'narrow_brown_spot': 5}\n"
     ]
    }
   ],
   "source": [
    "print(\"Class indices:\", train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the  number of samples that have been categorised as train and validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1680 training samples\n",
      "Found 420 validation samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {train_generator.samples} training samples\")\n",
    "print(f\"Found {validation_generator.samples} validation samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that  there are samples in the generators,we use assert to check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_generator.samples > 0, \"No training samples found\"\n",
    "assert validation_generator.samples > 0, \"No validation samples found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function for cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function call for the cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chand\\anaconda3\\envs\\exsel\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn_model = create_cnn_model(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate steps_per_epoch and validation_steps\n",
    "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
    "validation_steps = validation_generator.samples // validation_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_per_epoch: 52\n",
      "validation_steps: 13\n"
     ]
    }
   ],
   "source": [
    "print(f\"steps_per_epoch: {steps_per_epoch}\")\n",
    "print(f\"validation_steps: {validation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure steps_per_epoch and validation_steps are greater than 0\n",
    "if steps_per_epoch == 0:\n",
    "    steps_per_epoch = 1\n",
    "    print(\"Adjusted steps_per_epoch to 1\")\n",
    "\n",
    "if validation_steps == 0:\n",
    "    validation_steps = 1\n",
    "    print(\"Adjusted validation_steps to 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chand\\anaconda3\\envs\\exsel\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 2s/step - accuracy: 0.2908 - loss: 2.3711 - val_accuracy: 0.4928 - val_loss: 1.2433\n",
      "Epoch 2/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4062 - loss: 1.3600 - val_accuracy: 0.2500 - val_loss: 2.2647\n",
      "Epoch 3/35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chand\\anaconda3\\envs\\exsel\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - accuracy: 0.5461 - loss: 1.1480 - val_accuracy: 0.5673 - val_loss: 1.0850\n",
      "Epoch 4/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7188 - loss: 0.9782 - val_accuracy: 0.5000 - val_loss: 1.0895\n",
      "Epoch 5/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 0.6098 - loss: 0.9658 - val_accuracy: 0.5577 - val_loss: 1.0550\n",
      "Epoch 6/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5625 - loss: 1.1135 - val_accuracy: 0.7500 - val_loss: 0.6938\n",
      "Epoch 7/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - accuracy: 0.7055 - loss: 0.7941 - val_accuracy: 0.7115 - val_loss: 0.7930\n",
      "Epoch 8/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 788us/step - accuracy: 0.7500 - loss: 0.8031 - val_accuracy: 0.5000 - val_loss: 0.9770\n",
      "Epoch 9/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.7601 - loss: 0.6727 - val_accuracy: 0.6755 - val_loss: 0.7919\n",
      "Epoch 10/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952us/step - accuracy: 0.8750 - loss: 0.5959 - val_accuracy: 0.5000 - val_loss: 0.9335\n",
      "Epoch 11/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.7460 - loss: 0.6723 - val_accuracy: 0.6418 - val_loss: 0.9062\n",
      "Epoch 12/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 943us/step - accuracy: 0.7188 - loss: 0.8970 - val_accuracy: 0.5000 - val_loss: 0.9320\n",
      "Epoch 13/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 0.7587 - loss: 0.6601 - val_accuracy: 0.7139 - val_loss: 0.8089\n",
      "Epoch 14/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7188 - loss: 0.6934 - val_accuracy: 0.7500 - val_loss: 0.4983\n",
      "Epoch 15/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - accuracy: 0.8158 - loss: 0.5129 - val_accuracy: 0.7620 - val_loss: 0.6704\n",
      "Epoch 16/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 962us/step - accuracy: 0.7812 - loss: 0.4160 - val_accuracy: 0.7500 - val_loss: 0.6607\n",
      "Epoch 17/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - accuracy: 0.8526 - loss: 0.4051 - val_accuracy: 0.7284 - val_loss: 0.7695\n",
      "Epoch 18/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.6719 - val_accuracy: 0.5000 - val_loss: 1.5630\n",
      "Epoch 19/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - accuracy: 0.8301 - loss: 0.4959 - val_accuracy: 0.7788 - val_loss: 0.5835\n",
      "Epoch 20/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 979us/step - accuracy: 0.8438 - loss: 0.3227 - val_accuracy: 0.7500 - val_loss: 0.3259\n",
      "Epoch 21/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - accuracy: 0.8610 - loss: 0.3656 - val_accuracy: 0.8005 - val_loss: 0.4708\n",
      "Epoch 22/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9375 - loss: 0.1993 - val_accuracy: 1.0000 - val_loss: 0.2548\n",
      "Epoch 23/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - accuracy: 0.8708 - loss: 0.3368 - val_accuracy: 0.8221 - val_loss: 0.6138\n",
      "Epoch 24/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 881us/step - accuracy: 0.8750 - loss: 0.4347 - val_accuracy: 1.0000 - val_loss: 0.0254\n",
      "Epoch 25/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.8576 - loss: 0.3638 - val_accuracy: 0.8053 - val_loss: 0.5129\n",
      "Epoch 26/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - accuracy: 0.9062 - loss: 0.2392 - val_accuracy: 0.7500 - val_loss: 0.2550\n",
      "Epoch 27/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - accuracy: 0.8989 - loss: 0.2916 - val_accuracy: 0.7909 - val_loss: 0.5610\n",
      "Epoch 28/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8750 - loss: 0.2602 - val_accuracy: 1.0000 - val_loss: 0.1345\n",
      "Epoch 29/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 2s/step - accuracy: 0.9006 - loss: 0.2868 - val_accuracy: 0.7933 - val_loss: 0.6085\n",
      "Epoch 30/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.2914 - val_accuracy: 1.0000 - val_loss: 0.1528\n",
      "Epoch 31/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - accuracy: 0.9129 - loss: 0.2658 - val_accuracy: 0.7909 - val_loss: 0.5555\n",
      "Epoch 32/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 968us/step - accuracy: 0.7812 - loss: 0.5617 - val_accuracy: 0.7500 - val_loss: 0.6971\n",
      "Epoch 33/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - accuracy: 0.8973 - loss: 0.2531 - val_accuracy: 0.8293 - val_loss: 0.5446\n",
      "Epoch 34/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9375 - loss: 0.1486 - val_accuracy: 1.0000 - val_loss: 0.0221\n",
      "Epoch 35/35\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - accuracy: 0.9139 - loss: 0.2574 - val_accuracy: 0.8269 - val_loss: 0.4639\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history=cnn_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 573ms/step - accuracy: 0.8405 - loss: 0.4711\n",
      "Validation Accuracy: 83.89%\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "print(\"Evaluating model...\")\n",
    "val_loss, val_accuracy = cnn_model.evaluate(validation_generator, steps=validation_generator.samples //validation_generator.batch_size)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Load and Preprocess the Image using Pillow\n",
    "from PIL import Image\n",
    "def load_and_preprocess_image(image_path, target_size=(256, 256)):\n",
    "    # Load the image\n",
    "    img = Image.open(image_path)\n",
    "    # Resize the image\n",
    "    img = img.resize(target_size)\n",
    "    # Convert the image to a numpy array\n",
    "    img_array = np.array(img)\n",
    "    # Add batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    # Scale the image values to [0, 1]\n",
    "    img_array = img_array.astype('float32') / 255.\n",
    "    return img_array\n",
    "\n",
    "# Function to Predict the Class of an Image\n",
    "def predict_image_class(cnn_model, image_path, class_indices):\n",
    "    preprocessed_img = load_and_preprocess_image(image_path)\n",
    "    predictions = cnn_model.predict(preprocessed_img)\n",
    "    predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "    predicted_class_name = class_indices[predicted_class_index]\n",
    "    return predicted_class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from class indices to class names\n",
    "class_indices = {v: k for k, v in train_generator.class_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'bacterial_leaf_blight',\n",
       " 1: 'brown_spot',\n",
       " 2: 'healthy',\n",
       " 3: 'leaf_blast',\n",
       " 4: 'leaf_scald',\n",
       " 5: 'narrow_brown_spot'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the class names as json file\n",
    "import json\n",
    "json.dump(class_indices, open('class_indices.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "Predicted Class Name: leaf_blast\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "image_path = 'c:\\\\Users\\\\chand\\\\OneDrive\\\\Desktop\\\\exsel\\\\train_images\\\\bacterial_leaf_blight\\\\100133.jpg'\n",
    "predicted_class_name = predict_image_class(cnn_model, image_path, class_indices)\n",
    "\n",
    "# Output the result\n",
    "print(\"Predicted Class Name:\", predicted_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "cnn_model.save('C:\\\\Users\\\\chand\\\\OneDrive\\\\Desktop\\\\DP project\\\\Rice_leaf_disease_detection.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exsel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
